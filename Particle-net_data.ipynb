{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s: %(message)s')\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import sys, os, random, gzip\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "from sklearn import metrics\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import auc as skAUC\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, TensorBoard\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import time\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['t_allpar_new']>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = h5py.File(\"data/processed-pythia82-lhc13-all-pt1-50k-r1_h022_e0175_t220_nonu_withPars_truth_0.z\",'r',track_order=True)\n",
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('index', 'j_ptfrac', 'j_pt', 'j_eta', 'j_mass', 'j_tau1_b1', 'j_tau2_b1', 'j_tau3_b1', 'j_tau1_b2', 'j_tau2_b2', 'j_tau3_b2', 'j_tau32_b1', 'j_tau32_b2', 'j_zlogz', 'j_c1_b0', 'j_c1_b1', 'j_c1_b2', 'j_c2_b1', 'j_c2_b2', 'j_d2_b1', 'j_d2_b2', 'j_d2_a1_b1', 'j_d2_a1_b2', 'j_m2_b1', 'j_m2_b2', 'j_n2_b1', 'j_n2_b2', 'j_tau1_b1_mmdt', 'j_tau2_b1_mmdt', 'j_tau3_b1_mmdt', 'j_tau1_b2_mmdt', 'j_tau2_b2_mmdt', 'j_tau3_b2_mmdt', 'j_tau32_b1_mmdt', 'j_tau32_b2_mmdt', 'j_c1_b0_mmdt', 'j_c1_b1_mmdt', 'j_c1_b2_mmdt', 'j_c2_b1_mmdt', 'j_c2_b2_mmdt', 'j_d2_b1_mmdt', 'j_d2_b2_mmdt', 'j_d2_a1_b1_mmdt', 'j_d2_a1_b2_mmdt', 'j_m2_b1_mmdt', 'j_m2_b2_mmdt', 'j_n2_b1_mmdt', 'j_n2_b2_mmdt', 'j_mass_trim', 'j_mass_mmdt', 'j_mass_prun', 'j_mass_sdb2', 'j_mass_sdm1', 'j_multiplicity', 'j1_px', 'j1_py', 'j1_pz', 'j1_e', 'j1_pdgid', 'j1_erel', 'j1_pt', 'j1_ptrel', 'j1_eta', 'j1_etarel', 'j1_etarot', 'j1_phi', 'j1_phirel', 'j1_phirot', 'j1_deltaR', 'j1_costheta', 'j1_costhetarel', 'j1_e1mcosthetarel', 'j_g', 'j_q', 'j_w', 'j_z', 'j_t', 'j_undef', 'j_index')\n"
     ]
    }
   ],
   "source": [
    "treeArray = f['t_allpar_new'][()]\n",
    "print(treeArray.dtype.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "x = [len(list(y)) for _,y in itertools.groupby(treeArray['j_index'])]\n",
    "j_cIndex = np.array([],dtype='int8')\n",
    "\n",
    "for i in x:\n",
    "    new_jet_index = np.arange(i)\n",
    "    j_cIndex = np.append(j_cIndex, new_jet_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      0.,  128852.,  256625.,  384513.,  514097.,  641712.,\n",
       "        769858.,  896464., 1026061., 1156044., 1286398., 1417052.,\n",
       "       1544928., 1672454., 1802252., 1931669., 2060728., 2188800.,\n",
       "       2316946., 2444729., 2574941., 2702341., 2831864., 2960423.,\n",
       "       3087693., 3214838., 3342852., 3471194., 3598272., 3727227.,\n",
       "       3854143., 3981789., 4108865., 4236385., 4363616., 4492916.,\n",
       "       4620150., 4747591., 4875242., 5004078., 5131613.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_index = np.array([])\n",
    "for i in range(len(x)):\n",
    "    if i%2470 == 0:\n",
    "        split_index = np.append(split_index,np.sum(x[:i]))\n",
    "split_index = np.append(split_index,np.sum(x))\n",
    "split_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 暂存\n",
    "with h5py.File(\"data/j_cIndex.h5\",'w') as f1:\n",
    "    f1.create_dataset('j_cIndex',data=j_cIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_index = treeArray['index']\n",
    "ds_JCPT = treeArray['j1_pt']\n",
    "ds_JCETA = treeArray['j1_eta']\n",
    "ds_JCPHI = treeArray['j1_phi']\n",
    "ds_JCMASS = treeArray['j_mass']\n",
    "ds_Nconstituents = treeArray['j_multiplicity']\n",
    "ds_constituents_index = j_cIndex\n",
    "ds_JCDELTAETA = treeArray['j1_etarot']\n",
    "ds_JCDELTAPHI = treeArray['j1_phirot']\n",
    "ds_PT = treeArray['j_pt']\n",
    "ds_ETA = treeArray['j_eta']\n",
    "# ds_PHI = treeArray['j_phi']\n",
    "\n",
    "ds_label = np.vstack((treeArray['j_g'],treeArray['j_q'],treeArray['j_w'],treeArray['j_z'],treeArray['j_t'])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = split_index.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anacoda\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:3: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "Label = ['index', 'JCPT', 'JCETA', 'JCPHI', 'JCMASS', 'Nconstituents', 'constituents_index', 'JCDELTAETA', 'JCDELTAPHI', 'PT', 'ETA','label']\n",
    "for i in range(len(split_index)-1):\n",
    "    with h5py.File('data/Particle_%d.h5'%i) as f2:\n",
    "        for lb in Label:\n",
    "            f2.create_dataset(lb, data=eval('ds_%s'%lb)[split_index[i]:split_index[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### old\n",
    "# Label = ['index', 'JCPT', 'JCETA', 'JCPHI', 'JCMASS', 'Nconstituents', 'constituents_index', 'JCDELTAETA', 'JCDELTAPHI', 'PT', 'ETA','label']\n",
    "# with h5py.File('data/data_Particle-net.h5','w') as f_new:\n",
    "#     for lb in Label:\n",
    "#         f_new.create_dataset(lb, data=eval('ds_%s'%lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------calculate jet constituents energy, if you already had this information don't use it.\n",
    "def jce_np(jcpt, jceta, jcmass):\n",
    "    jcp = jcpt*np.cosh(jceta)\n",
    "    return (jcp**2+jcmass**2)**0.5\n",
    "\n",
    "\n",
    "#-------- attach integers to your keys  this example is : 'index' -> 0-th data, 'JCPT' ->1-th data\n",
    "# and 'index' is event index\n",
    "# 'JCPT' is constituents transvers momentum (PT)\n",
    "# 'JCETA' is constituents ETA\n",
    "# 'JCPHI' is constituents angle around beam axis\n",
    "# 'Ncontituents' is Number of constituents in the jet which the constitunt belong\n",
    "# 'constiteunts_index' is constituents index in each jet ('constiteunts' is typo)\n",
    "# 'JCDELTAETA' is constituents ETA related to jet axis\n",
    "# 'JCDELTAPHI' is constituents ETA related to jet axis\n",
    "# 'PT' is transvers momentum (PT) of jet which which the constitunt belong\n",
    "# 'ETA' is ETA of jet which which the constitunt belong\n",
    "# 'PHi' is PHI of jet which which the constitunt belong\n",
    "\n",
    "\n",
    "_index, _jcpt, _jceta, _jcphi, _jcmass, _Ncons, _consindex, _jcdelteta, _jcdeltphi, _jpt, _jeta = (i for i in range(11))\n",
    "Labels = ['index', 'JCPT', 'JCETA', 'JCPHI', 'JCMASS', 'Nconstituents', 'constituents_index', 'JCDELTAETA', 'JCDELTAPHI', 'PT', 'ETA']\n",
    "\n",
    "##--------------------------------------------------h5py file to ParticleNet input structure\n",
    "def h5_to_data(h5path):\n",
    "    Data = {'mask':[], 'points':[], 'features':[]}\n",
    "    f = h5py.File(h5path,'r')\n",
    "    fc = np.array([f[lb][()] for lb in Labels])\n",
    "    fc = fc.transpose((1,0))\n",
    "    j0 = fc[0][_index]\n",
    "    \n",
    "    JCE = jce_np(fc[:,_jcpt], fc[:,_jceta], fc[:,_jcphi])\n",
    "    logpt = np.log(fc[:,_jcpt])\n",
    "    loge = np.log(JCE)\n",
    "    relatpt = fc[:,_jcpt]/fc[:,_jpt]\n",
    "    mask, features, points = np.zeros((100,1)), np.zeros((100,5)), np.zeros((100,2)) # prepare constituents list\n",
    "    Nfc = len(fc)\n",
    "    for j in range(len(fc)):\n",
    "        if fc[j][_Ncons]>100:\n",
    "            if j< Nfc-1:\n",
    "                j0 = fc[j+1][_index]\n",
    "            continue\n",
    "        if fc[j][_index]!=j0:\n",
    "            j0 = fc[j][_index]\n",
    "            Data['mask'].append(mask)\n",
    "            Data['points'].append(points)\n",
    "            Data['features'].append(features)\n",
    "            mask, features, points = np.zeros((100,1)), np.zeros((100,5)), np.zeros((100,2)) # prepare constituents list\n",
    "            continue\n",
    "        jc = int(fc[j][_consindex])\n",
    "#         jce = JCE[j]\n",
    "        \n",
    "        points[jc] = np.array([fc[j][_jcdelteta], fc[j][_jcdeltphi] ])\n",
    "        mask[jc] = logpt[j]\n",
    "        features[jc] = np.array([logpt[j], loge[j], fc[j][_jcdelteta], fc[j][_jcdeltphi], relatpt[j]])\n",
    "    return Data\n",
    "\n",
    "##==========================================================================================\n",
    "##----------merging 2 sample with ParticleNet input structure\n",
    "def merging(gg,qq):\n",
    "    total={}\n",
    "    total['mask']=gg[\"mask\"]+qq[\"mask\"]\n",
    "    total['features']=gg[\"features\"]+qq[\"features\"]\n",
    "    total['points']=gg['points']+qq['points']\n",
    "    return total\n",
    "##--------- seperate inputs to training, validation and testing with ratio you give\n",
    "def separatedata(features_list,y,rateval,ratetest):\n",
    "    features_train, features_test, features_val={},{},{}\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    mask = features_list[\"mask\"]\n",
    "    features = features_list[\"features\"]\n",
    "    points = features_list[\"points\"]\n",
    "    X_ind = [i for i in range(len(y))]\n",
    "    X_train, X_ind, y_train, y_ind = train_test_split(X_ind, y, test_size=rateval+ratetest)\n",
    "    N=int(len(X_ind)*rateval/(rateval+ratetest))\n",
    "    X_val, X_test = X_ind[:N], X_ind[N:]\n",
    "    y_val, y_test = y_ind[:N], y_ind[N:]\n",
    "    features_train['mask']=np.array([mask[i] for i in X_train])\n",
    "    features_train['features']=np.array([features[i] for i in X_train])\n",
    "    features_train['points']=np.array([points[i] for i in X_train])\n",
    "    \n",
    "    features_test['mask']=np.array([mask[i] for i in X_test])\n",
    "    features_test['features']=np.array([features[i] for i in X_test])\n",
    "    features_test['points']=np.array([points[i] for i in X_test])\n",
    "    \n",
    "    features_val['mask']=np.array([mask[i] for i in X_val])\n",
    "    features_val['features']=np.array([features[i] for i in X_val])\n",
    "    features_val['points']=np.array([points[i] for i in X_val])\n",
    "    \n",
    "    return features_train, features_val, features_test,np.array(y_train), np.array(y_val), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████▌                                                             | 10/40 [00:17<00:53,  1.77s/it]"
     ]
    }
   ],
   "source": [
    "N = 40\n",
    "h5Path = \"data/data_Particle-net.h5\"\n",
    "Data = {'mask':[], 'points':[], 'features':[]}\n",
    "for i in tqdm(range(N)):\n",
    "    h5Path = \"data/Particle_\"+str(i)+\".h5\"\n",
    "    data = h5_to_data(h5Path)\n",
    "    Data = merging(Data,data)\n",
    "print(\"check shape: \",Data['mask'][0].shape,Data['points'][0].shape,Data['features'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5py.File(h5Path)['label'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## binary calssification: \n",
    "# y=[[0,1] for i in range(len(Data['mask']))]+[[1,0] for i in range(len(Data2['mask']))] \n",
    "# total = merging(Data, Data2)\n",
    "\n",
    "y = h5py.File(h5Path)['label'][()]\n",
    "\n",
    "# del Data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = separatedata(Data,y,0.25,0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the shape\n",
    "print([X_train[i][0].shape for i in ['mask', 'points', 'features']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'models')\n",
    "from tf_keras_model import get_particle_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shapes={'points': X_train['points'][0].shape, 'features': X_train['features'][0].shape, 'mask': X_train['mask'][0].shape}\n",
    "num_classes = 5\n",
    "model = get_particle_net(num_classes, input_shapes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train ,y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs, \n",
    "          validation_data=(X_val, y_val),\n",
    "          shuffle=True ,\n",
    "#           callbacks=callbacks\n",
    "         )\n",
    "model.evaluate(X_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
