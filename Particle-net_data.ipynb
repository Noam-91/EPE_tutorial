{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s: %(message)s')\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import sys, os, random, gzip\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "from sklearn import metrics\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import auc as skAUC\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, TensorBoard\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import time\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['t_allpar_new']>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = h5py.File(\"data/processed-pythia82-lhc13-all-pt1-50k-r1_h022_e0175_t220_nonu_withPars_truth_0.z\",'r',track_order=True)\n",
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('index', 'j_ptfrac', 'j_pt', 'j_eta', 'j_mass', 'j_tau1_b1', 'j_tau2_b1', 'j_tau3_b1', 'j_tau1_b2', 'j_tau2_b2', 'j_tau3_b2', 'j_tau32_b1', 'j_tau32_b2', 'j_zlogz', 'j_c1_b0', 'j_c1_b1', 'j_c1_b2', 'j_c2_b1', 'j_c2_b2', 'j_d2_b1', 'j_d2_b2', 'j_d2_a1_b1', 'j_d2_a1_b2', 'j_m2_b1', 'j_m2_b2', 'j_n2_b1', 'j_n2_b2', 'j_tau1_b1_mmdt', 'j_tau2_b1_mmdt', 'j_tau3_b1_mmdt', 'j_tau1_b2_mmdt', 'j_tau2_b2_mmdt', 'j_tau3_b2_mmdt', 'j_tau32_b1_mmdt', 'j_tau32_b2_mmdt', 'j_c1_b0_mmdt', 'j_c1_b1_mmdt', 'j_c1_b2_mmdt', 'j_c2_b1_mmdt', 'j_c2_b2_mmdt', 'j_d2_b1_mmdt', 'j_d2_b2_mmdt', 'j_d2_a1_b1_mmdt', 'j_d2_a1_b2_mmdt', 'j_m2_b1_mmdt', 'j_m2_b2_mmdt', 'j_n2_b1_mmdt', 'j_n2_b2_mmdt', 'j_mass_trim', 'j_mass_mmdt', 'j_mass_prun', 'j_mass_sdb2', 'j_mass_sdm1', 'j_multiplicity', 'j1_px', 'j1_py', 'j1_pz', 'j1_e', 'j1_pdgid', 'j1_erel', 'j1_pt', 'j1_ptrel', 'j1_eta', 'j1_etarel', 'j1_etarot', 'j1_phi', 'j1_phirel', 'j1_phirot', 'j1_deltaR', 'j1_costheta', 'j1_costhetarel', 'j1_e1mcosthetarel', 'j_g', 'j_q', 'j_w', 'j_z', 'j_t', 'j_undef', 'j_index')\n"
     ]
    }
   ],
   "source": [
    "treeArray = f['t_allpar_new'][()]\n",
    "print(treeArray.dtype.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "x = [len(list(y)) for _,y in itertools.groupby(treeArray['j_index'])]\n",
    "j_cIndex = np.array([],dtype='int8')\n",
    "\n",
    "for i in x:\n",
    "    new_jet_index = np.arange(i)\n",
    "    j_cIndex = np.append(j_cIndex, new_jet_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      0.,  128852.,  256625.,  384513.,  514097.,  641712.,\n",
       "        769858.,  896464., 1026061., 1156044., 1286398., 1417052.,\n",
       "       1544928., 1672454., 1802252., 1931669., 2060728., 2188800.,\n",
       "       2316946., 2444729., 2574941., 2702341., 2831864., 2960423.,\n",
       "       3087693., 3214838., 3342852., 3471194., 3598272., 3727227.,\n",
       "       3854143., 3981789., 4108865., 4236385., 4363616., 4492916.,\n",
       "       4620150., 4747591., 4875242., 5004078., 5131613.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_index = np.array([])\n",
    "for i in range(len(x)):\n",
    "    if i%2470 == 0:\n",
    "        split_index = np.append(split_index,np.sum(x[:i]))\n",
    "split_index = np.append(split_index,np.sum(x))\n",
    "split_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 暂存\n",
    "with h5py.File(\"data/j_cIndex.h5\",'w') as f1:\n",
    "    f1.create_dataset('j_cIndex',data=j_cIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_index = treeArray['index']\n",
    "ds_JCPT = treeArray['j1_pt']\n",
    "ds_JCETA = treeArray['j1_eta']\n",
    "ds_JCPHI = treeArray['j1_phi']\n",
    "ds_JCMASS = treeArray['j_mass']\n",
    "ds_Nconstituents = treeArray['j_multiplicity']\n",
    "ds_constituents_index = j_cIndex\n",
    "ds_JCDELTAETA = treeArray['j1_etarot']\n",
    "ds_JCDELTAPHI = treeArray['j1_phirot']\n",
    "ds_PT = treeArray['j_pt']\n",
    "ds_ETA = treeArray['j_eta']\n",
    "# ds_PHI = treeArray['j_phi']\n",
    "\n",
    "ds_label = np.vstack((treeArray['j_g'],treeArray['j_q'],treeArray['j_w'],treeArray['j_z'],treeArray['j_t'])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = split_index.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anacoda\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:3: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "Label = ['index', 'JCPT', 'JCETA', 'JCPHI', 'JCMASS', 'Nconstituents', 'constituents_index', 'JCDELTAETA', 'JCDELTAPHI', 'PT', 'ETA','label']\n",
    "for i in range(len(split_index)-1):\n",
    "    with h5py.File('data/Particle_%d.h5'%i) as f2:\n",
    "        for lb in Label:\n",
    "            f2.create_dataset(lb, data=eval('ds_%s'%lb)[split_index[i]:split_index[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### old\n",
    "# Label = ['index', 'JCPT', 'JCETA', 'JCPHI', 'JCMASS', 'Nconstituents', 'constituents_index', 'JCDELTAETA', 'JCDELTAPHI', 'PT', 'ETA','label']\n",
    "# with h5py.File('data/data_Particle-net.h5','w') as f_new:\n",
    "#     for lb in Label:\n",
    "#         f_new.create_dataset(lb, data=eval('ds_%s'%lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------calculate jet constituents energy, if you already had this information don't use it.\n",
    "def jce_np(jcpt, jceta, jcmass):\n",
    "    jcp = jcpt*np.cosh(jceta)\n",
    "    return (jcp**2+jcmass**2)**0.5\n",
    "\n",
    "\n",
    "#-------- attach integers to your keys  this example is : 'index' -> 0-th data, 'JCPT' ->1-th data\n",
    "# and 'index' is event index\n",
    "# 'JCPT' is constituents transvers momentum (PT)\n",
    "# 'JCETA' is constituents ETA\n",
    "# 'JCPHI' is constituents angle around beam axis\n",
    "# 'Ncontituents' is Number of constituents in the jet which the constitunt belong\n",
    "# 'constiteunts_index' is constituents index in each jet ('constiteunts' is typo)\n",
    "# 'JCDELTAETA' is constituents ETA related to jet axis\n",
    "# 'JCDELTAPHI' is constituents ETA related to jet axis\n",
    "# 'PT' is transvers momentum (PT) of jet which which the constitunt belong\n",
    "# 'ETA' is ETA of jet which which the constitunt belong\n",
    "# 'PHi' is PHI of jet which which the constitunt belong\n",
    "\n",
    "\n",
    "_index, _jcpt, _jceta, _jcphi, _jcmass, _Ncons, _consindex, _jcdelteta, _jcdeltphi, _jpt, _jeta = (i for i in range(11))\n",
    "Labels = ['index', 'JCPT', 'JCETA', 'JCPHI', 'JCMASS', 'Nconstituents', 'constituents_index', 'JCDELTAETA', 'JCDELTAPHI', 'PT', 'ETA']\n",
    "\n",
    "##--------------------------------------------------h5py file to ParticleNet input structure\n",
    "def h5_to_data(h5path):\n",
    "    Data = {'mask':[], 'points':[], 'features':[]}\n",
    "    f = h5py.File(h5path,'r')\n",
    "    fc = np.array([f[lb][()] for lb in Labels])\n",
    "    fc = fc.transpose((1,0))\n",
    "    j0 = fc[0][_index]\n",
    "    \n",
    "    JCE = jce_np(fc[:,_jcpt], fc[:,_jceta], fc[:,_jcphi])\n",
    "    logpt = np.log(fc[:,_jcpt])\n",
    "    loge = np.log(JCE)\n",
    "    relatpt = fc[:,_jcpt]/fc[:,_jpt]\n",
    "    mask, features, points = np.zeros((100,1)), np.zeros((100,5)), np.zeros((100,2)) # prepare constituents list\n",
    "    Nfc = len(fc)\n",
    "    for j in range(len(fc)):\n",
    "        if fc[j][_Ncons]>100:\n",
    "            if j< Nfc-1:\n",
    "                j0 = fc[j+1][_index]\n",
    "            continue\n",
    "        if fc[j][_index]!=j0:\n",
    "            j0 = fc[j][_index]\n",
    "            Data['mask'].append(mask)\n",
    "            Data['points'].append(points)\n",
    "            Data['features'].append(features)\n",
    "            mask, features, points = np.zeros((100,1)), np.zeros((100,5)), np.zeros((100,2)) # prepare constituents list\n",
    "            continue\n",
    "        jc = int(fc[j][_consindex])\n",
    "#         jce = JCE[j]\n",
    "        \n",
    "        points[jc] = np.array([fc[j][_jcdelteta], fc[j][_jcdeltphi] ])\n",
    "        mask[jc] = logpt[j]\n",
    "        features[jc] = np.array([logpt[j], loge[j], fc[j][_jcdelteta], fc[j][_jcdeltphi], relatpt[j]])\n",
    "    return Data\n",
    "\n",
    "##==========================================================================================\n",
    "##----------merging 2 sample with ParticleNet input structure\n",
    "def merging(gg,qq):\n",
    "    total={}\n",
    "    total['mask']=gg[\"mask\"]+qq[\"mask\"]\n",
    "    total['features']=gg[\"features\"]+qq[\"features\"]\n",
    "    total['points']=gg['points']+qq['points']\n",
    "    return total\n",
    "##--------- seperate inputs to training, validation and testing with ratio you give\n",
    "def separatedata(features_list,y,rateval,ratetest):\n",
    "    features_train, features_test, features_val={},{},{}\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    mask = features_list[\"mask\"]\n",
    "    features = features_list[\"features\"]\n",
    "    points = features_list[\"points\"]\n",
    "    X_ind = [i for i in range(len(y))]\n",
    "    X_train, X_ind, y_train, y_ind = train_test_split(X_ind, y, test_size=rateval+ratetest)\n",
    "    N=int(len(X_ind)*rateval/(rateval+ratetest))\n",
    "    X_val, X_test = X_ind[:N], X_ind[N:]\n",
    "    y_val, y_test = y_ind[:N], y_ind[N:]\n",
    "    features_train['mask']=np.array([mask[i] for i in X_train])\n",
    "    features_train['features']=np.array([features[i] for i in X_train])\n",
    "    features_train['points']=np.array([points[i] for i in X_train])\n",
    "    \n",
    "    features_test['mask']=np.array([mask[i] for i in X_test])\n",
    "    features_test['features']=np.array([features[i] for i in X_test])\n",
    "    features_test['points']=np.array([points[i] for i in X_test])\n",
    "    \n",
    "    features_val['mask']=np.array([mask[i] for i in X_val])\n",
    "    features_val['features']=np.array([features[i] for i in X_val])\n",
    "    features_val['points']=np.array([points[i] for i in X_val])\n",
    "    \n",
    "    return features_train, features_val, features_test,np.array(y_train), np.array(y_val), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|█████████████████████████████████████████████████████████████████████▋            | 34/40 [02:18<00:38,  6.44s/it]"
     ]
    }
   ],
   "source": [
    "N = 40\n",
    "h5Path = \"data/data_Particle-net.h5\"\n",
    "Data = {'mask':[], 'points':[], 'features':[]}\n",
    "for i in tqdm(range(N)):\n",
    "    h5Path = \"data/Particle_\"+str(i)+\".h5\"\n",
    "    data = h5_to_data(h5Path)\n",
    "    Data = merging(Data,data)\n",
    "print(\"check shape: \",Data['mask'][0].shape,Data['points'][0].shape,Data['features'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h5py' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4268f5d47acb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh5Path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'h5py' is not defined"
     ]
    }
   ],
   "source": [
    "h5py.File(h5Path)['label'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.4 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-8fff59f82535>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# del gg, qq, eventg, eventq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# del Data, Data2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseparatedata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-7e6ea3674d13>\u001b[0m in \u001b[0;36mseparatedata\u001b[1;34m(features_list, y, rateval, ratetest)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mpoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"points\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mX_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrateval\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mratetest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m     \u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_ind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrateval\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrateval\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mratetest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_ind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_ind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anacoda\\envs\\tf2\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2120\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2121\u001b[0m     n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,\n\u001b[1;32m-> 2122\u001b[1;33m                                               default_test_size=0.25)\n\u001b[0m\u001b[0;32m   2123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\Anacoda\\envs\\tf2\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   1803\u001b[0m             \u001b[1;34m'resulting train set will be empty. Adjust any of the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1804\u001b[0m             'aforementioned parameters.'.format(n_samples, test_size,\n\u001b[1;32m-> 1805\u001b[1;33m                                                 train_size)\n\u001b[0m\u001b[0;32m   1806\u001b[0m         )\n\u001b[0;32m   1807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.4 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# ## binary calssification: \n",
    "# y=[[0,1] for i in range(len(Data['mask']))]+[[1,0] for i in range(len(Data2['mask']))] \n",
    "# total = merging(Data, Data2)\n",
    "\n",
    "y = h5py.File(h5Path)['label'][()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# del gg, qq, eventg, eventq\n",
    "# del Data, Data2\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = separatedata(total,y,0.3,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.18.5'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
